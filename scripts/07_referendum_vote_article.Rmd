---
title: "How many people voted on the 1st of October independence referendum?"
subtitle: "A bootstrap approach using a quota sample survey."
author: Josep Espasa Reig
tags: ["R", "survey", "variance estimation", "bootstrap"]
topics: "Referendum"
output: html_document

---

# Acknowledgements

This blog couldn't have been writen without the scripts published by the authors of Sturgis et al 2017 [^1] and the responses to my technical questions provided by people at ['Centre d'Estudis d'Opinió' (CEO)](http://ceo.gencat.cat/ca/inici/index.html).

# How many people voted?

The official data offered by the Catalan Government a few days after the independence referendum registered a turnover of almost 2.3 million votes or 43.03% of the census [^2]. This count, however, missed the polls that were seized by Spanish police forces. The by then Catalan Government spokesman, said that, under 'normal circumstances', participation could have easily reached 55% [^3]. 

The real number of votes is thus still unknown, and very little data from surveys on this issue is publicly available. The main exception to this is the ['Centre d'Estudis d'Opinió' (CEO)](http://ceo.gencat.cat/ca/inici/index.html) 'barometer' survey. To my knowledge, this poll is the only one that asked respondents about their participation on the 1-O referendum.

The CEO survey, collected data during the last two weeks of that month and used a quota sampling with sample allocation proportional to population estimates [^4]. The proportional allocation meant that the sample size in three out of the four provinces was too small to compute reliable estimates. The sample size of the Province of Barcelona, however, was close to 1,000 respondents and should allow for a decent calculation [^5]. 

According to the official information from the Catalan Government the province had a turnover of 41.2% or around 1,6 million votes. As shown in Figure 1, my estimate from CEO survey data would be that the turnover was most likely between 50.6% and 56.8%. This would mean that between 2 and 2.26 million people voted in this province. 

Even knowing that some polls were taken away by Spanish police forces, the difference between the official information and the CEO survey is large. Thus, estimates from the survey should be taken with a grain of salt. We know that people tend to lie about their participation in elections, stating that they voted when they actually didn't[^6]. Moreover, a quota sample like the one used by CEO is based on assumptions that might have been violated in this case. These assumptions and the exact methods used to compute the estimates are explained in the next sections in a slightly more technical way.

```{r echo=FALSE, message=FALSE, warning=FALSE}

rm(list = ls())

library(here)
library(magrittr)
library(tidyverse)

p_1 <- read_rds(here("outputs", "plots", "vote_estimates_intervals.rds"))


p_1
```

# Quota sampling

Before explaining the particluarities of quota sampling, let me give a brief overview about the concepts of probability and non-probability sampling.

## Probability vs non-probability sampling

Concisely explained, there are two types of survey sampling: probability sampling and non-probability sampling. In the survey industry the first one is still the gold standard. In probability sampling, we draw a random sample of units from a population. We know the probability of each unit being selected. The probability of inclusin should not be equal to 0 for any unit, having all of them a chance to be in the survey. The great advantage of probability sampling is that they are unbiased. This is, if the sampling was to be repeated many times, the expected value of the estimates from the samples would be identical to the value for the whole population. Thus, the sample will tend to be (on average) a small-scale representation of the population from which we can obtain unbiased estimates. 

Three main issues of probabilty samples are coverage error, non-response error and their cost. The first two are problems that affect the capacity of the sample to properly represent the population. Coverage error is caused by some units not being in the list from which the random sample is selected[^7]. Non-response error is when the survey fails to collect responses for some of the units. This increases the uncertainty of estimates due to smaller sample sizes and risks biasing the survey as certain profiles of respondents might have a larger propensity to respond than others. Some statisticians have pointed that, due to non-response, no survey is really a probability sample [^8]. The same voices say that the low response rates that we see in certain research areas make it 

The falling response rates 

Non-probabililty samples are those in which the selection of units is not randomised according to principles of probability theory. There is a wide variety of non-probability sample methods [^9]. They are extensively used in market research but considered of poor quality in many fields. 


Because we know the probability of getting each sample we select, we can also calculate a sampling
error for the results. The sampling error tells us the amount of variation in the results due to the
sampling alone. It gives a measure of the quality of the sample design, and of the survey results.

coverage, non-response, costs

Making inferences for any probability or non-probability survey requires some reliance
on modeling assumptions. requiring significant statistical expertise.

What assumption are they based on
Why computing the variance with a srs might be wrong
recruitment method
* Methods used for variance estimation 




Notes: 

- why not all provinces? quotas too small

-measurement error 
-Explain weighting. What it does and how it was done.
- note on CEO survey: unfit for 


[^1]: Sturgis, P., Kuha, J., Baker, N., Callegaro, M., Fisher, S., Green, J., Jennings, W., Lauderdale, B. E., and Smith, P. (2017) An assessment of the causes of the errors in the 2015 UK General Election opinion polls. Journal of the Royal Statistical Society A. Article available [here](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssa.12329) and code available [here](http://stats.lse.ac.uk/kuha/Publications/publications.html) 

[^2]: Official data on the referendum participation can be found [here](https://estaticos.elperiodico.com/resources/pdf/4/3/1507302086634.pdf?_ga=2.11952052.1654533961.1528484435-1600052709.1528484435).

[^3]: https://www.elperiodico.com/es/politica/20171006/resultados-referendum-cataluna-2017-6319340.

[^4]: The technical specifications of the survey can be found here: 
http://upceo.ceo.gencat.cat/wsceop/6408/Abstract%20in%20English%20-863.pdf (in English) 
and with greater detail here:
http://upceo.ceo.gencat.cat/wsceop/6408/Fitxa%20t%C3%A8cnica%20-863.pdf (in Catalan)

[^5]: See section XXX TO DO: on why I didn't use all provinces.

[^6]: See examples of research done on this issue in Holbrook,A.L. & Krosnick, J.A.(2009) Social Desirability Bias in Voter Turnout Reports: Tests Using the Item Count Technique. Available here: https://web.stanford.edu/dept/communication/faculty/krosnick/Turnout%20Overreporting%20-%20ICT%20Only%20-%20Final.pdf

[^7]: This list is commonly known as [sampling frame](https://stats.oecd.org/glossary/detail.asp?ID=2379).

[^8]: See for example: [This article by Andrew Gelman](https://www.washingtonpost.com/news/monkey-cage/wp/2014/04/11/when-should-we-trust-polls-from-non-probability-samples/?noredirect=on&utm_term=.0cc27f8322c8)

[^9]: Some of these designs are explained and commented in the report from the American Association for Public Opinion Research (AAPOR) on [Non-Probability Sampling](https://www.aapor.org/AAPOR_Main/media/MainSiteFiles/NPS_TF_Report_Final_7_revised_FNL_6_22_13.pdf)
